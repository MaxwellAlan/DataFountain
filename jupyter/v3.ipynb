{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"../resource/PINGAN-2018-train_demo.csv\"\n",
    "path_test = \"../resource/PINGAN-2018-test_demo.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_list = ['CALLSTATE', 'DIRECTION', 'HEIGHT', 'LATITUDE', 'LONGITUDE', 'SPEED',\n",
    "       'TERMINALNO', 'TIME', 'TRIP_ID', 'Y', 'time', 'date', 'hour', 'minute',\n",
    "       'trip_max', 'lon_max', 'lon_min', 'lon', 'lat_max', 'lat_min', 'lat',\n",
    "       'heg_max', 'heg_min', 'heg_mean', 'heg', 'vol', 'sp_max', 'sp_mean',\n",
    "       'call0', 'call1', 'call_ratio_0', 'call_ratio_1', 'dis', 'ave_dri_time', 'dri_time']\n",
    "\n",
    "use_feature_list = [\n",
    "    'trip_max', 'lon_max', 'lon_min', 'lon_50','lat_max', 'lat_min', 'lat_50', 'heg_max',\n",
    "    'heg_min', 'heg_mean', 'heg_50', 'sp_max', 'sp_mean', 'sp_50', 'dis', 'avg_dis',\n",
    "    'dri_time', 'ave_dri_time', 'dri_time_trip_max']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看内存\n",
    "keydic = {\"MemTotal\":\"TotalMem\",\"MemFree\":\"FreeMem\",\"MemAvailable\":\"AvaiableMem\",\"Cached\":\"Cached\"}\n",
    "def command(command):\n",
    "    p = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    resultDic = {}\n",
    "    for line in p.stdout.readlines():\n",
    "        line = str(line,encoding=\"utf-8\")\n",
    "        result = re.split(\"\\s*\",line)\n",
    "        if result[0][:-1] in keydic:\n",
    "            resultDic[keydic[result[0][:-1]]] = \"%.2f\" %(int(result[1])/(1024**2))\n",
    "    return resultDic\n",
    "\n",
    "def load_data(path_train,path_test):\n",
    "    train_data = pd.read_csv(path_train)\n",
    "    test_data = pd.read_csv(path_test)\n",
    "    return train_data,test_data\n",
    "\n",
    "\n",
    "def read_csv():\n",
    "    \"\"\"\n",
    "    文件读取模块，头文件见columns.\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # for filename in os.listdir(path_train):\n",
    "    tempdata = pd.read_csv(path_train)\n",
    "    tempdata.columns = [\"TERMINALNO\", \"TIME\", \"TRIP_ID\", \"LONGITUDE\", \"LATITUDE\", \"DIRECTION\", \"HEIGHT\", \"SPEED\",\n",
    "                        \"CALLSTATE\", \"Y\"]\n",
    "\n",
    "#时间处理\n",
    "def time_datetime(value):\n",
    "    format = '%Y%m%d%H%M'\n",
    "    value = time.localtime(value)\n",
    "    dt = time.strftime(format, value)\n",
    "    return int(dt)\n",
    "\n",
    "def time_date(value):\n",
    "    format = '%Y%m%d'\n",
    "    value = time.localtime(value)\n",
    "    dt = time.strftime(format, value)\n",
    "    return int(dt)\n",
    "\n",
    "def time_hour(value):\n",
    "    format = '%H'\n",
    "    value = time.localtime(value)\n",
    "    dt = time.strftime(format, value)\n",
    "    return int(dt)\n",
    "\n",
    "def time_minute(value):\n",
    "    format = '%M'\n",
    "    value = time.localtime(value)\n",
    "    dt = time.strftime(format, value)\n",
    "    return int(dt)\n",
    "#驾驶时长转换\n",
    "def f(x):\n",
    "    if x >= 20:\n",
    "        return 0\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(command(\"cat /proc/meminfo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 总特征DF\n",
    "feature = pd.DataFrame()\n",
    "\n",
    "# 特征一,trip_max\n",
    "train_data = pd.read_csv(path_train,usecols=['TERMINALNO', 'TIME', 'TRIP_ID'])\n",
    "test_data = pd.read_csv(path_test,usecols=['TERMINALNO', 'TIME', 'TRIP_ID'])\n",
    "\n",
    "TRAIN_ID_MAX = train_data['TERMINALNO'].max() + 10\n",
    "test_data['TERMINALNO'] = test_data['TERMINALNO'] + TRAIN_ID_MAX\n",
    "data = pd.concat([train_data, test_data])\n",
    "data.drop_duplicates(inplace=True, subset=['TERMINALNO','TIME'])\n",
    "\n",
    "feature[['TERMINALNO', 'trip_max']] = data['TRIP_ID'].groupby(data['TERMINALNO']).max().reset_index()\n",
    "del train_data, test_data, data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 特征二,lon_max lon_min lon_50\n",
    "train_data = pd.read_csv(path_train,usecols=['TERMINALNO', 'LONGITUDE'])\n",
    "test_data = pd.read_csv(path_test,usecols=['TERMINALNO', 'LONGITUDE'])\n",
    "\n",
    "test_data['TERMINALNO'] = test_data['TERMINALNO'] + TRAIN_ID_MAX\n",
    "data = pd.concat([train_data, test_data])\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "feature[['TERMINALNO', 'lon_max']] = pd.DataFrame(data['LONGITUDE'].groupby(data['TERMINALNO']).max()).reset_index()\n",
    "feature[['TERMINALNO', 'lon_min']] = pd.DataFrame(data['LONGITUDE'].groupby(data['TERMINALNO']).min()).reset_index()\n",
    "feature[['TERMINALNO', 'lon_50']] = pd.DataFrame(data['LONGITUDE'].groupby(data['TERMINALNO']).quantile()).reset_index()\n",
    "del train_data, test_data, data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 特征三,lat_max lat_min lat_50\n",
    "train_data = pd.read_csv(path_train,usecols=['TERMINALNO', 'LATITUDE'])\n",
    "test_data = pd.read_csv(path_test,usecols=['TERMINALNO', 'LATITUDE'])\n",
    "\n",
    "test_data['TERMINALNO'] = test_data['TERMINALNO'] + TRAIN_ID_MAX\n",
    "data = pd.concat([train_data, test_data])\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "feature[['TERMINALNO', 'lat_max']] = pd.DataFrame(data['LATITUDE'].groupby(data['TERMINALNO']).max()).reset_index()\n",
    "feature[['TERMINALNO', 'lat_min']] = pd.DataFrame(data['LATITUDE'].groupby(data['TERMINALNO']).min()).reset_index()\n",
    "feature[['TERMINALNO', 'lat_50']] = pd.DataFrame(data['LATITUDE'].groupby(data['TERMINALNO']).quantile()).reset_index()\n",
    "del train_data, test_data, data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 特征四,heg_max heg_min heg_mean heg_50\n",
    "train_data = pd.read_csv(path_train,usecols=['TERMINALNO', 'HEIGHT'])\n",
    "test_data = pd.read_csv(path_test,usecols=['TERMINALNO', 'HEIGHT'])\n",
    "\n",
    "test_data['TERMINALNO'] = test_data['TERMINALNO'] + TRAIN_ID_MAX\n",
    "data = pd.concat([train_data, test_data])\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.fillna(0.0, inplace=True)\n",
    "\n",
    "feature[['TERMINALNO', 'heg_max']] = pd.DataFrame(data['HEIGHT'].groupby(data['TERMINALNO']).max()).reset_index()\n",
    "feature[['TERMINALNO', 'heg_min']] = pd.DataFrame(data['HEIGHT'].groupby(data['TERMINALNO']).min()).reset_index()\n",
    "feature[['TERMINALNO', 'heg_mean']] = pd.DataFrame(data['HEIGHT'].groupby(data['TERMINALNO']).mean()).reset_index()\n",
    "feature[['TERMINALNO', 'heg_50']] = pd.DataFrame(data['HEIGHT'].groupby(data['TERMINALNO']).quantile()).reset_index()\n",
    "del train_data, test_data, data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 特征五,sp_max sp_mean sp_50\n",
    "train_data = pd.read_csv(path_train,usecols=['TERMINALNO', 'SPEED'])\n",
    "test_data = pd.read_csv(path_test,usecols=['TERMINALNO', 'SPEED'])\n",
    "\n",
    "test_data['TERMINALNO'] = test_data['TERMINALNO'] + TRAIN_ID_MAX\n",
    "data = pd.concat([train_data, test_data])\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.fillna(0.0, inplace=True)\n",
    "\n",
    "feature[['TERMINALNO', 'sp_max']] = pd.DataFrame(data['SPEED'].groupby(data['TERMINALNO']).max()).reset_index()\n",
    "feature[['TERMINALNO', 'sp_mean']] = pd.DataFrame(data['SPEED'].groupby(data['TERMINALNO']).mean()).reset_index()\n",
    "feature[['TERMINALNO', 'sp_50']] = pd.DataFrame(data['SPEED'].groupby(data['TERMINALNO']).quantile()).reset_index()\n",
    "del train_data, test_data, data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 特征七, 'dis', 'avg_dis'\n",
    "train_data = pd.read_csv(path_train,usecols=['TERMINALNO','TIME', 'LONGITUDE', 'LATITUDE'])\n",
    "test_data = pd.read_csv(path_test,usecols=['TERMINALNO','TIME', 'LONGITUDE', 'LATITUDE'])\n",
    "\n",
    "test_data['TERMINALNO'] = test_data['TERMINALNO'] + TRAIN_ID_MAX\n",
    "data = pd.concat([train_data, test_data])\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.fillna(0.0, inplace=True)\n",
    "\n",
    "# 每个用户按时间排序\n",
    "data.sort_values(by=['TERMINALNO', 'TIME'], inplace=True)\n",
    "# 计算经纬度差(未分Trip)\n",
    "data['difflat'] = data.groupby(['TERMINALNO'])['LATITUDE'].diff()\n",
    "data['difflon'] = data.groupby(['TERMINALNO'])['LONGITUDE'].diff()\n",
    "# 对每个用户的第一个经纬度差置0\n",
    "data.fillna(0.0, inplace=True)\n",
    "# 计算单个距离\n",
    "data['dis2'] = data['difflat'] ** 2 + data['difflon'] ** 2\n",
    "data['dis'] = data['dis2'].apply(sqrt)\n",
    "feature[['TERMINALNO', 'dis']] = pd.DataFrame(data['dis'].groupby(data['TERMINALNO']).sum()).reset_index()\n",
    "feature['avg_dis'] = feature['dis'] / feature['trip_max']\n",
    "\n",
    "del train_data, test_data, data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 特征八, dri_time ave_dri_time dri_time_trip_max\n",
    "train_data = pd.read_csv(path_train, usecols=['TERMINALNO', 'TIME', 'TRIP_ID'])\n",
    "test_data = pd.read_csv(path_test,usecols=['TERMINALNO', 'TIME', 'TRIP_ID'])\n",
    "\n",
    "test_data['TERMINALNO'] = test_data['TERMINALNO'] + TRAIN_ID_MAX\n",
    "data = pd.concat([train_data, test_data])\n",
    "data.drop_duplicates(subset=['TERMINALNO', 'TIME'], inplace=True)\n",
    "data.fillna(0.0, inplace=True)\n",
    "\n",
    "# 按 TERMINALNO 和 time 排序\n",
    "data.sort_values(['TERMINALNO', 'TIME'], inplace=True)\n",
    "data['diff_time'] = data.groupby(['TERMINALNO'])['TIME'].diff()\n",
    "data.fillna(0.0, inplace=True)\n",
    "data['diff_time'] = data['diff_time'].apply(f)\n",
    "dri_time_trip_max = pd.DataFrame()\n",
    "dri_time_trip_max = pd.DataFrame(data.groupby(['TERMINALNO','TRIP_ID'])['diff_time'].sum()).reset_index()\n",
    "\n",
    "# 计算驾驶总时长,用户单段最大驾驶时长(trip分割不准确)\n",
    "feature[['TERMINALNO', 'dri_time']] = pd.DataFrame(data['diff_time'].groupby(data['TERMINALNO']).sum()).reset_index()\n",
    "feature[['TERMINALNO', 'dri_time_trip_max']] = pd.DataFrame(dri_time_trip_max['diff_time'].groupby(dri_time_trip_max['TERMINALNO']).max()).reset_index()\n",
    "feature['ave_dri_time'] = feature['dri_time'] / feature['trip_max']\n",
    "\n",
    "del train_data, test_data, data, dri_time_trip_max\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data normalization..\n",
      "Feature End. feature shape:(200, 20)\n",
      "generate train & test set\n"
     ]
    }
   ],
   "source": [
    "# 归一化\n",
    "feature['trip_max'] = feature['trip_max'].apply(\n",
    "    lambda x: (x - feature['trip_max'].min()) / (feature['trip_max'].max() - feature['trip_max'].min()))\n",
    "feature['lon_max'] = feature['lon_max'].apply(\n",
    "    lambda x: (x - feature['lon_max'].min()) / (feature['lon_max'].max() - feature['lon_max'].min()))\n",
    "feature['lon_min'] = feature['lon_min'].apply(\n",
    "    lambda x: (x - feature['lon_min'].min()) / (feature['lon_min'].max() - feature['lon_min'].min()))\n",
    "feature['lon_50'] = feature['lon_50'].apply(\n",
    "    lambda x: (x - feature['lon_50'].min()) / (feature['lon_50'].max() - feature['lon_50'].min()))\n",
    "feature['lat_min'] = feature['lat_min'].apply(\n",
    "    lambda x: (x - feature['lat_min'].min()) / (feature['lat_min'].max() - feature['lat_min'].min()))\n",
    "feature['lat_max'] = feature['lat_max'].apply(\n",
    "    lambda x: (x - feature['lat_max'].min()) / (feature['lat_max'].max() - feature['lat_max'].min()))\n",
    "feature['lat_50'] = feature['lat_50'].apply(\n",
    "    lambda x: (x - feature['lat_50'].min()) / (feature['lat_50'].max() - feature['lat_50'].min()))\n",
    "feature['heg_min'] = feature['heg_min'].apply(\n",
    "    lambda x: (x - feature['heg_min'].min()) / (feature['heg_min'].max() - feature['heg_min'].min()))\n",
    "feature['heg_max'] = feature['heg_max'].apply(\n",
    "    lambda x: (x - feature['heg_max'].min()) / (feature['heg_max'].max() - feature['heg_max'].min()))\n",
    "feature['heg_50'] = feature['heg_50'].apply(\n",
    "    lambda x: (x - feature['heg_50'].min()) / (feature['heg_50'].max() - feature['heg_50'].min()))\n",
    "feature['heg_mean'] = feature['heg_mean'].apply(\n",
    "    lambda x: (x - feature['heg_mean'].min()) / (feature['heg_mean'].max() - feature['heg_mean'].min()))\n",
    "feature['sp_50'] = feature['sp_50'].apply(\n",
    "    lambda x: (x - feature['sp_50'].min()) / (feature['sp_50'].max() - feature['sp_50'].min()))\n",
    "feature['sp_max'] = feature['sp_max'].apply(\n",
    "    lambda x: (x - feature['sp_max'].min()) / (feature['sp_max'].max() - feature['sp_max'].min()))\n",
    "feature['sp_mean'] = feature['sp_mean'].apply(\n",
    "    lambda x: (x - feature['sp_mean'].min()) / (feature['sp_mean'].max() - feature['sp_mean'].min()))\n",
    "feature['ave_dri_time'] = feature['ave_dri_time'].apply(\n",
    "    lambda x: (x - feature['ave_dri_time'].min()) / (feature['ave_dri_time'].max() - feature['ave_dri_time'].min()))\n",
    "feature['dri_time'] = feature['dri_time'].apply(\n",
    "    lambda x: (x - feature['dri_time'].min()) / (feature['dri_time'].max() - feature['dri_time'].min()))\n",
    "feature['dis'] = feature['dis'].apply(\n",
    "    lambda x: (x - feature['dis'].min()) / (feature['dis'].max() - feature['dis'].min()))\n",
    "feature['dri_time_trip_max'] = feature['dri_time_trip_max'].apply(\n",
    "    lambda x: (x - feature['dri_time_trip_max'].min()) / (feature['dri_time_trip_max'].max() - feature['dri_time_trip_max'].min()))\n",
    "feature['avg_dis'] = feature['avg_dis'].apply(\n",
    "    lambda x: (x - feature['avg_dis'].min()) / (feature['avg_dis'].max() - feature['avg_dis'].min()))\n",
    "print(\"data normalization..\")\n",
    "print(\"Feature End. feature shape:\" + str(feature.shape))\n",
    "print(\"generate train & test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " # train_Y\n",
    "train_Y = pd.read_csv(path_train, usecols=['TERMINALNO', 'Y'])\n",
    "train_Y.drop_duplicates(inplace=True)\n",
    "# Y值变换\n",
    "train_Y.loc[:, 'Y'][train_Y['Y'] <= 0] = 0.00001\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "Y_arrary = np.array(train_Y['Y'])\n",
    "y, _ = stats.boxcox(Y_arrary)\n",
    "# for i in range(len(y)):\n",
    "train_Y.loc[:, 'Y'] = y\n",
    "del Y_arrary, y, _\n",
    "gc.collect()\n",
    "\n",
    "feature = pd.merge(feature, train_Y, how='left', on='TERMINALNO')\n",
    "train = feature[0:len(train_Y)]\n",
    "test = feature[len(train_Y):]\n",
    "train['Y'] = train['Y'].apply(lambda x: ((x - train['Y'].min()) / (train['Y'].max() - train['Y'].min())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_train_shape:(80, 21)  train_val_shape:(20, 21)\n",
      "model training\n"
     ]
    }
   ],
   "source": [
    "# 训练集和验证集划分\n",
    "train_train, train_val = train_test_split(train, test_size=0.2, random_state=42)\n",
    "print(\"train_train_shape:\"+str(train_train.shape)+\"  train_val_shape:\"+str(train_val.shape))\n",
    "print(\"model training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's l2: 0.127541\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2]\tvalid_0's l2: 0.127185\n",
      "[3]\tvalid_0's l2: 0.127148\n",
      "[4]\tvalid_0's l2: 0.12698\n",
      "[5]\tvalid_0's l2: 0.126757\n",
      "[6]\tvalid_0's l2: 0.126655\n",
      "[7]\tvalid_0's l2: 0.126502\n",
      "[8]\tvalid_0's l2: 0.125333\n",
      "[9]\tvalid_0's l2: 0.124278\n",
      "[10]\tvalid_0's l2: 0.124151\n",
      "[11]\tvalid_0's l2: 0.123211\n",
      "[12]\tvalid_0's l2: 0.123114\n",
      "[13]\tvalid_0's l2: 0.122278\n",
      "[14]\tvalid_0's l2: 0.122205\n",
      "[15]\tvalid_0's l2: 0.121064\n",
      "[16]\tvalid_0's l2: 0.122048\n",
      "[17]\tvalid_0's l2: 0.122226\n",
      "[18]\tvalid_0's l2: 0.121563\n",
      "[19]\tvalid_0's l2: 0.122814\n",
      "[20]\tvalid_0's l2: 0.123806\n",
      "[21]\tvalid_0's l2: 0.123189\n",
      "[22]\tvalid_0's l2: 0.124417\n",
      "[23]\tvalid_0's l2: 0.123478\n",
      "[24]\tvalid_0's l2: 0.124485\n",
      "[25]\tvalid_0's l2: 0.123832\n",
      "[26]\tvalid_0's l2: 0.125031\n",
      "[27]\tvalid_0's l2: 0.124484\n",
      "[28]\tvalid_0's l2: 0.125486\n",
      "[29]\tvalid_0's l2: 0.125283\n",
      "[30]\tvalid_0's l2: 0.124784\n",
      "[31]\tvalid_0's l2: 0.125967\n",
      "[32]\tvalid_0's l2: 0.125499\n",
      "[33]\tvalid_0's l2: 0.126508\n",
      "[34]\tvalid_0's l2: 0.125955\n",
      "[35]\tvalid_0's l2: 0.127108\n",
      "[36]\tvalid_0's l2: 0.126794\n",
      "[37]\tvalid_0's l2: 0.126363\n",
      "[38]\tvalid_0's l2: 0.12625\n",
      "[39]\tvalid_0's l2: 0.125854\n",
      "[40]\tvalid_0's l2: 0.126854\n",
      "[41]\tvalid_0's l2: 0.126602\n",
      "[42]\tvalid_0's l2: 0.126128\n",
      "[43]\tvalid_0's l2: 0.127216\n",
      "[44]\tvalid_0's l2: 0.126851\n",
      "[45]\tvalid_0's l2: 0.127811\n",
      "[46]\tvalid_0's l2: 0.127585\n",
      "[47]\tvalid_0's l2: 0.128186\n",
      "[48]\tvalid_0's l2: 0.12784\n",
      "[49]\tvalid_0's l2: 0.126953\n",
      "[50]\tvalid_0's l2: 0.126626\n",
      "[51]\tvalid_0's l2: 0.127025\n",
      "[52]\tvalid_0's l2: 0.127985\n",
      "[53]\tvalid_0's l2: 0.127134\n",
      "[54]\tvalid_0's l2: 0.127126\n",
      "[55]\tvalid_0's l2: 0.125849\n",
      "[56]\tvalid_0's l2: 0.126781\n",
      "[57]\tvalid_0's l2: 0.126492\n",
      "[58]\tvalid_0's l2: 0.126446\n",
      "[59]\tvalid_0's l2: 0.126315\n",
      "[60]\tvalid_0's l2: 0.127298\n",
      "[61]\tvalid_0's l2: 0.127843\n",
      "[62]\tvalid_0's l2: 0.127361\n",
      "[63]\tvalid_0's l2: 0.127797\n",
      "[64]\tvalid_0's l2: 0.128713\n",
      "[65]\tvalid_0's l2: 0.127949\n",
      "[66]\tvalid_0's l2: 0.127501\n",
      "[67]\tvalid_0's l2: 0.128401\n",
      "[68]\tvalid_0's l2: 0.12727\n",
      "[69]\tvalid_0's l2: 0.12731\n",
      "[70]\tvalid_0's l2: 0.128186\n",
      "[71]\tvalid_0's l2: 0.127481\n",
      "[72]\tvalid_0's l2: 0.12723\n",
      "[73]\tvalid_0's l2: 0.127734\n",
      "[74]\tvalid_0's l2: 0.127794\n",
      "[75]\tvalid_0's l2: 0.128658\n",
      "[76]\tvalid_0's l2: 0.128237\n",
      "[77]\tvalid_0's l2: 0.129124\n",
      "[78]\tvalid_0's l2: 0.128064\n",
      "[79]\tvalid_0's l2: 0.128893\n",
      "[80]\tvalid_0's l2: 0.128488\n",
      "[81]\tvalid_0's l2: 0.128564\n",
      "[82]\tvalid_0's l2: 0.127572\n",
      "[83]\tvalid_0's l2: 0.128424\n",
      "[84]\tvalid_0's l2: 0.128081\n",
      "[85]\tvalid_0's l2: 0.127704\n",
      "[86]\tvalid_0's l2: 0.128519\n",
      "[87]\tvalid_0's l2: 0.127878\n",
      "[88]\tvalid_0's l2: 0.127496\n",
      "[89]\tvalid_0's l2: 0.128291\n",
      "[90]\tvalid_0's l2: 0.127369\n",
      "[91]\tvalid_0's l2: 0.127451\n",
      "[92]\tvalid_0's l2: 0.128237\n",
      "[93]\tvalid_0's l2: 0.127638\n",
      "[94]\tvalid_0's l2: 0.127266\n",
      "[95]\tvalid_0's l2: 0.127837\n",
      "[96]\tvalid_0's l2: 0.12772\n",
      "[97]\tvalid_0's l2: 0.127804\n",
      "[98]\tvalid_0's l2: 0.127451\n",
      "[99]\tvalid_0's l2: 0.128213\n",
      "[100]\tvalid_0's l2: 0.12743\n",
      "[101]\tvalid_0's l2: 0.12798\n",
      "[102]\tvalid_0's l2: 0.127413\n",
      "[103]\tvalid_0's l2: 0.127083\n",
      "[104]\tvalid_0's l2: 0.127196\n",
      "[105]\tvalid_0's l2: 0.127926\n",
      "[106]\tvalid_0's l2: 0.127102\n",
      "[107]\tvalid_0's l2: 0.12678\n",
      "[108]\tvalid_0's l2: 0.126451\n",
      "[109]\tvalid_0's l2: 0.126879\n",
      "[110]\tvalid_0's l2: 0.126953\n",
      "[111]\tvalid_0's l2: 0.126243\n",
      "[112]\tvalid_0's l2: 0.126939\n",
      "[113]\tvalid_0's l2: 0.126267\n",
      "[114]\tvalid_0's l2: 0.12639\n",
      "[115]\tvalid_0's l2: 0.127081\n",
      "[116]\tvalid_0's l2: 0.126771\n",
      "[117]\tvalid_0's l2: 0.127335\n",
      "[118]\tvalid_0's l2: 0.126812\n",
      "[119]\tvalid_0's l2: 0.127045\n",
      "[120]\tvalid_0's l2: 0.126742\n",
      "[121]\tvalid_0's l2: 0.126877\n",
      "[122]\tvalid_0's l2: 0.127547\n",
      "[123]\tvalid_0's l2: 0.126798\n",
      "[124]\tvalid_0's l2: 0.127031\n",
      "[125]\tvalid_0's l2: 0.127129\n",
      "[126]\tvalid_0's l2: 0.126636\n",
      "[127]\tvalid_0's l2: 0.126342\n",
      "[128]\tvalid_0's l2: 0.12678\n",
      "[129]\tvalid_0's l2: 0.126141\n",
      "[130]\tvalid_0's l2: 0.126678\n",
      "[131]\tvalid_0's l2: 0.126921\n",
      "[132]\tvalid_0's l2: 0.127059\n",
      "[133]\tvalid_0's l2: 0.127707\n",
      "[134]\tvalid_0's l2: 0.127011\n",
      "[135]\tvalid_0's l2: 0.126733\n",
      "[136]\tvalid_0's l2: 0.127246\n",
      "[137]\tvalid_0's l2: 0.126643\n",
      "[138]\tvalid_0's l2: 0.126912\n",
      "[139]\tvalid_0's l2: 0.127063\n",
      "[140]\tvalid_0's l2: 0.12678\n",
      "[141]\tvalid_0's l2: 0.126872\n",
      "[142]\tvalid_0's l2: 0.126605\n",
      "[143]\tvalid_0's l2: 0.126843\n",
      "[144]\tvalid_0's l2: 0.126983\n",
      "[145]\tvalid_0's l2: 0.127073\n",
      "[146]\tvalid_0's l2: 0.127593\n",
      "[147]\tvalid_0's l2: 0.126946\n",
      "[148]\tvalid_0's l2: 0.126361\n",
      "[149]\tvalid_0's l2: 0.126098\n",
      "[150]\tvalid_0's l2: 0.126269\n",
      "[151]\tvalid_0's l2: 0.12601\n",
      "[152]\tvalid_0's l2: 0.126494\n",
      "[153]\tvalid_0's l2: 0.126237\n",
      "[154]\tvalid_0's l2: 0.12567\n",
      "[155]\tvalid_0's l2: 0.126288\n",
      "[156]\tvalid_0's l2: 0.12646\n",
      "[157]\tvalid_0's l2: 0.12697\n",
      "[158]\tvalid_0's l2: 0.126354\n",
      "[159]\tvalid_0's l2: 0.125808\n",
      "[160]\tvalid_0's l2: 0.125694\n",
      "[161]\tvalid_0's l2: 0.12583\n",
      "[162]\tvalid_0's l2: 0.126401\n",
      "[163]\tvalid_0's l2: 0.126143\n",
      "[164]\tvalid_0's l2: 0.126327\n",
      "[165]\tvalid_0's l2: 0.126223\n",
      "[166]\tvalid_0's l2: 0.126732\n",
      "[167]\tvalid_0's l2: 0.126483\n",
      "[168]\tvalid_0's l2: 0.125398\n",
      "[169]\tvalid_0's l2: 0.125861\n",
      "[170]\tvalid_0's l2: 0.125604\n",
      "[171]\tvalid_0's l2: 0.125359\n",
      "[172]\tvalid_0's l2: 0.124792\n",
      "[173]\tvalid_0's l2: 0.124703\n",
      "[174]\tvalid_0's l2: 0.125195\n",
      "[175]\tvalid_0's l2: 0.12532\n",
      "[176]\tvalid_0's l2: 0.12508\n",
      "[177]\tvalid_0's l2: 0.125544\n",
      "[178]\tvalid_0's l2: 0.125023\n",
      "[179]\tvalid_0's l2: 0.125204\n",
      "[180]\tvalid_0's l2: 0.125126\n",
      "[181]\tvalid_0's l2: 0.12501\n",
      "[182]\tvalid_0's l2: 0.12514\n",
      "[183]\tvalid_0's l2: 0.12559\n",
      "[184]\tvalid_0's l2: 0.126077\n",
      "[185]\tvalid_0's l2: 0.125562\n",
      "[186]\tvalid_0's l2: 0.126042\n",
      "[187]\tvalid_0's l2: 0.125028\n",
      "[188]\tvalid_0's l2: 0.124963\n",
      "[189]\tvalid_0's l2: 0.124721\n",
      "[190]\tvalid_0's l2: 0.124972\n",
      "[191]\tvalid_0's l2: 0.124578\n",
      "[192]\tvalid_0's l2: 0.124053\n",
      "[193]\tvalid_0's l2: 0.123817\n",
      "[194]\tvalid_0's l2: 0.123345\n",
      "[195]\tvalid_0's l2: 0.123472\n",
      "[196]\tvalid_0's l2: 0.123909\n",
      "[197]\tvalid_0's l2: 0.124378\n",
      "[198]\tvalid_0's l2: 0.124363\n",
      "[199]\tvalid_0's l2: 0.124566\n",
      "[200]\tvalid_0's l2: 0.12447\n",
      "[201]\tvalid_0's l2: 0.123969\n",
      "[202]\tvalid_0's l2: 0.124221\n",
      "[203]\tvalid_0's l2: 0.12473\n",
      "[204]\tvalid_0's l2: 0.124494\n",
      "[205]\tvalid_0's l2: 0.124919\n",
      "[206]\tvalid_0's l2: 0.125389\n",
      "[207]\tvalid_0's l2: 0.12492\n",
      "[208]\tvalid_0's l2: 0.124869\n",
      "[209]\tvalid_0's l2: 0.124638\n",
      "[210]\tvalid_0's l2: 0.124755\n",
      "[211]\tvalid_0's l2: 0.124523\n",
      "[212]\tvalid_0's l2: 0.124298\n",
      "[213]\tvalid_0's l2: 0.123817\n",
      "[214]\tvalid_0's l2: 0.124013\n",
      "[215]\tvalid_0's l2: 0.124477\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's l2: 0.121064\n",
      "lat_min              9\n",
      "lat_50               7\n",
      "heg_min              5\n",
      "avg_dis              4\n",
      "heg_50               3\n",
      "dis                  1\n",
      "lat_max              1\n",
      "dri_time_trip_max    0\n",
      "lon_max              0\n",
      "lon_min              0\n",
      "lon_50               0\n",
      "heg_mean             0\n",
      "heg_max              0\n",
      "ave_dri_time         0\n",
      "sp_max               0\n",
      "sp_mean              0\n",
      "sp_50                0\n",
      "dri_time             0\n",
      "trip_max             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#模型训练\n",
    "lgbmodel = lgb.LGBMRegressor(\n",
    "    boosting_type='gbdt',\n",
    "    objective='regression',\n",
    "    num_leaves=63,\n",
    "    max_depth=8,\n",
    "    n_estimators=20000,\n",
    "    learning_rate=0.05,\n",
    "    # n_jobs=20,\n",
    "    random_state=42\n",
    ")\n",
    "lgbmodel.fit(\n",
    "    X=train_train[use_feature_list],\n",
    "    y=train_train['Y'],\n",
    "    eval_set=(train_val[use_feature_list], train_val['Y']),\n",
    "    early_stopping_rounds=200,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "fea_imp = pd.Series(lgbmodel.feature_importances_,use_feature_list).sort_values(ascending=False)\n",
    "print(fea_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
